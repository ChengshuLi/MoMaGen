{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MoMaGen","text":"<p>Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</p> <p></p>"},{"location":"#what-is-momagen","title":"What is MoMaGen?","text":"<p>MoMaGen is a framework for generating several robot demonstrations using a single human-collected demonstration for complex bimanual mobile manipulation tasks. By considering soft constraints\u2014like maintaining visibility of the object during navigation\u2014and hard constraints\u2014such as guaranteeing the object is reachable upon arrival\u2014it produces multi-step demonstrations that are both feasible and useful for policy learning.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Data Generation</li> <li>GitHub Repository</li> </ul>"},{"location":"#license","title":"License","text":"<p>See LICENSE for details.</p>"},{"location":"installation/","title":"Installation","text":"<p>Prerequisites</p> <p>Before starting, ensure you have:</p> <ul> <li>Python 3.10+</li> <li>Conda (recommended for environment management)</li> </ul>"},{"location":"installation/#step-1-clone-repository","title":"Step 1: Clone Repository","text":"<p>Clone the repository with all submodules:</p> <pre><code>git clone --recurse-submodules https://github.com/ChengshuLi/MoMaGen.git\ncd MoMaGen\n</code></pre>"},{"location":"installation/#step-2-create-conda-environment","title":"Step 2: Create Conda Environment","text":"<p>Set up a new conda environment:</p> <pre><code>conda create -n momagen python=3.10\nconda activate momagen\n</code></pre>"},{"location":"installation/#step-3-install-dependencies","title":"Step 3: Install Dependencies","text":""},{"location":"installation/#install-momagen","title":"Install MoMaGen","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#install-behavior-1k-and-dependencies","title":"Install BEHAVIOR-1K and Dependencies","text":"<pre><code>cd BEHAVIOR-1K\n./setup.sh --omnigibson --bddl --joylo --dataset --primitives\ncd ..\n</code></pre>"},{"location":"installation/#install-robomimic","title":"Install Robomimic","text":"<pre><code>cd robomimic\npip install -e .\ncd ..\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Verify Installation</p> <p>To verify the installation, check that the required packages are available:</p> <pre><code>python -c \"import momagen; import omnigibson; import robomimic; print('Installation successful')\"\n</code></pre> <p>If successful, you should see: <code>Installation successful</code></p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <p>Submodules not cloned</p> <pre><code>git submodule update --init --recursive\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete, proceed to the data generation tutorial.</p>"},{"location":"tutorials/custom-tasks/","title":"Creating Custom Tasks","text":"<p>Learn how to define and generate demonstrations for your own custom mobile manipulation tasks.</p>"},{"location":"tutorials/custom-tasks/#step-1-define-a-new-behavior-task-with-bddl","title":"Step 1: Define a New BEHAVIOR Task with BDDL","text":"<p>First, review how BEHAVIOR tasks are defined here:</p> <p>\ud83d\udc49 BEHAVIOR Task Definition</p> <p>At the bottom of that page, you\u2019ll find a section on \u201cCreating Custom Tasks\u201d.</p> <p>You can define a custom task for MoMaGen in the same way you define a new BEHAVIOR task.</p> <p>In this guide, we\u2019ll walk through an example of creating a task called <code>datagen_picking_up_trash</code>, a simplified version of the existing <code>picking_up_trash</code> task. (You can read more about the original task here.)</p> <p>As described in the \u201cCreating Custom Tasks\u201d section, we\u2019ll follow these steps:</p> <p>Create a task directory:</p> <pre><code>mkdir BEHAVIOR-1K/bddl/bddl/activity_definitions/datagen_picking_up_trash\n</code></pre> <p>Create a task definition file:</p> <pre><code>touch BEHAVIOR-1K/bddl/bddl/activity_definitions/datagen_picking_up_trash/problem0.bddl\n</code></pre> <p>Define the task:</p> <pre><code>(define (problem datagen_picking_up_trash-0)\n    (:domain omnigibson)\n\n    (:objects\n        ashcan.n.01_1 - ashcan.n.01\n        can__of__soda.n.01_1 - can__of__soda.n.01\n        table.n.02_1 - table.n.02\n        floor.n.01_1 - floor.n.01\n        agent.n.01_1 - agent.n.01\n    )\n\n    (:init \n        (ontop ashcan.n.01_1 floor.n.01_1) \n        (ontop can__of__soda.n.01_1 table.n.02_1) \n        (inroom table.n.02_1 kitchen) \n        (inroom floor.n.01_1 kitchen) \n        (ontop agent.n.01_1 floor.n.01_1)\n    )\n\n    (:goal \n        (and \n            (inside ?can__of__soda.n.01_1 ?ashcan.n.01_1)\n        )\n    )\n)\n</code></pre> <p>To verify that the task is correctly defined, load it and enable online object sampling:</p> <pre><code>import omnigibson as og\ncfg = {\n    # ... standard configuration ...\n    \"task\": {\n        \"type\": \"BehaviorTask\",\n        \"activity_name\": \"datagen_picking_up_trash\",  # Your custom task\n        \"activity_definition_id\": 0,\n        \"activity_instance_id\": 0,\n        \"online_object_sampling\": True,\n    },\n}\n\nenv = og.Environment(configs=cfg)\n</code></pre>"},{"location":"tutorials/custom-tasks/#step-2-sample-a-task-instance","title":"Step 2: Sample a Task Instance","text":"<p>Open <code>BEHAVIOR-1K/OmniGibson/omnigibson/sampling/sample_b1k_tasks.py</code> and locate the <code>TASK_CUSTOM_LISTS</code> dictionary. Add an entry for your task, for example:</p> <pre><code>    (\"datagen_picking_up_trash\", \"house_double_floor_lower\"): {\n        \"whitelist\": {\n            \"can__of__soda.n.01\": {\n                \"can_of_soda\": {\n                    \"itolcg\": None,\n                    \"lugwcz\": None,\n                    \"opivig\": None\n                }\n            },\n            \"ashcan.n.01\": {\n                \"trash_can\": {\n                    \"wkxtxh\": None\n                }\n            }\n        },\n        \"blacklist\": None,\n    },\n</code></pre> <p>This dictionary specifies which objects to include in your task.</p> <ul> <li>Whitelist lets you specify exactly which categories and models should be used. For <code>can__of__soda.n.01</code>, we choose the <code>can_of_soda</code> category and whitelist three models (<code>itolcg</code>, <code>lugwcz</code>, <code>opivig</code>) to sample from. For <code>ashcan.n.01</code>, we choose the <code>trash_can</code> category and whitelist a single model (<code>wkxtxh</code>).</li> <li>Blacklist can be used to exclude specific categories or models.</li> </ul> <p>To find valid category and model names, use the Behavior Knowledge Base.</p> <p>Once that\u2019s set up, you can sample your task:</p> <pre><code>python -m omnigibson.sampling.sample_b1k_tasks --scene_model house_double_floor_lower --activities datagen_picking_up_trash\n</code></pre> <p>Whether sampling succeeds or fails, you\u2019ll be dropped into a PDB breakpoint where you can inspect the sampled task instance.</p> <p>If sampling succeeds, you can visualize it in the simulator:</p> <pre><code>for _ in range(1000): og.sim.render()\n</code></pre> <p>This lets you move the camera and inspect object placements.</p> <p>If sampling fails, adjust your whitelist/blacklist or the initial conditions in your BDDL task. Common failure cases include:</p> <ul> <li>No valid object instances found for one or more objects.</li> <li>No valid placement found, e.g., the object doesn\u2019t fit inside or can\u2019t stably rest on another object.</li> </ul> <p>Once you\u2019re satisfied with the sampled instance, press <code>c</code> in PDB to continue execution and save it to disk. By default, the instance will be saved to:</p> <pre><code>BEHAVIOR-1K/datasets/2025-challenge-task-instances/scenes/house_double_floor_lower/json/house_double_floor_lower_task_datagen_picking_up_trash_0_0_template.json\n</code></pre> <p>\u2705 Congrats \u2014 you\u2019ve successfully defined and sampled a custom task!</p>"},{"location":"tutorials/custom-tasks/#step-3-collect-source-demonstrations","title":"Step 3: Collect Source Demonstrations","text":"<p>To collect source demonstrations, you can use one of the following input devices:</p> <ul> <li>JoyLo (used for MoMaGen source demos and the BEHAVIOR Challenge dataset)</li> <li>Keyboard</li> <li>Spacemouse</li> <li>VR</li> </ul>"},{"location":"tutorials/custom-tasks/#using-joylo","title":"Using JoyLo","text":"<p>You can learn more about JoyLo here: \ud83d\udc49 JoyLo Overview</p>"},{"location":"tutorials/custom-tasks/#1-hardware-setup","title":"1. Hardware Setup","text":"<p>Follow this guide to set up the hardware: \ud83d\udc49 JoyLo Hardware Guide</p>"},{"location":"tutorials/custom-tasks/#2-software-setup","title":"2. Software Setup","text":"<p>After setting up the hardware, continue with the software instructions on this page: \ud83d\udc49 JoyLo Software Guide</p> <p>When you reach Step 5 of the JoyLo guide, you can launch the recording process.</p> <p>In one terminal, run:</p> <pre><code>python BEHAVIOR-1K/joylo/experiments/launch_nodes.py \\\n  --recording_path /PATH_TO_MOMAGEN/momagen/datasets/source_og/r1_picking_up_trash.hdf5 \\\n  --task_name datagen_picking_up_trash\n</code></pre> <p>In another terminal, run:</p> <pre><code>python joylo/experiments/run_joylo.py \\\n  --gello_model r1 \\\n  --joint_config_file joint_config_{YOUR_GELLO_SET_NAME}.yaml\n</code></pre>"},{"location":"tutorials/custom-tasks/#3-saving-the-recording","title":"3. Saving the Recording","text":"<p>Once you finish collecting your source demonstration:</p> <ol> <li>Focus your cursor on the GUI window.</li> <li>Press <code>ESC</code>.</li> <li>The recording will be saved automatically, and the program will exit.</li> </ol>"},{"location":"tutorials/custom-tasks/#using-other-devices","title":"Using Other Devices","text":"<p>The other teleop devices (such as keyboard, spacemouse and VR) are interfaced using telemoma. Run  <pre><code>python BEHAVIOR-1K/OmniGibson/omnigibson/examples/teleoperation/robot_teleoperate_demo.py\n</code></pre> amd refer to this file for futher details.</p>"},{"location":"tutorials/custom-tasks/#step-4-process-source-demonstrations","title":"Step 4: Process Source Demonstrations","text":"<p>In this step, you will annotate the source demonstration and generate the configuration files that contain the information required for MoMaGen data generation. </p> <p>Concretely, you need to create a new json file in <code>MoMaGen/momagen/datasets/base_configs</code>. You can start by copying the contents of an existing JSON file from the <code>base_configs</code> folder and then modify it to suit your new task. Note that only the manipulation phases need to be annotated.</p>"},{"location":"tutorials/custom-tasks/#1-set-basic-fields","title":"1. Set Basic Fields","text":"<p>Modify the <code>name</code> to your task name and set <code>filter_key</code> to <code>null</code> (unless you have multiple episodes in your HDF5 file and would like to specify which episode to use for data generation).</p>"},{"location":"tutorials/custom-tasks/#2-define-task-phases","title":"2. Define Task Phases","text":"<p>A phase represents a semantic step in a multi-step, long-horizon task. For instance, a phase could be picking a cup, putting an apple in a bowl, pouring water in a mug, opening a drawer.</p>"},{"location":"tutorials/custom-tasks/#3-specify-phase-type","title":"3. Specify Phase Type","text":"<p>For each phase, specify its <code>type</code>. This can be either <code>uncoordinated</code> or <code>coordinated</code>.</p>"},{"location":"tutorials/custom-tasks/#4-define-subtasks-for-each-phase","title":"4. Define Subtasks for Each Phase","text":"<p>Next, define the subtasks for each phase and for each arm (left and right).</p> <p>A subtask represents the portion of a phase that can be broken down into:</p> <ul> <li>A free-space motion part, and</li> <li>A contact-rich part.</li> </ul> <p>Example: For the open drawer task:</p> <ul> <li>The free-space motion involves moving the gripper from its starting pose to near the handle.</li> <li>The contact-rich part involves grasping the handle and performing the motion to open the drawer.</li> </ul> <p>In all existing MoMaGen tasks, each phase typically has one subtask. You may define multiple subtasks if desired. Each subtask must contain at most one free-space and at most one contact-rich segment.</p>"},{"location":"tutorials/custom-tasks/#5-specify-object-references","title":"5. Specify Object References","text":"<p>For each arm in a subtask, specify:</p> <ul> <li><code>object_ref</code>: The reference object for this arm.  </li> <li><code>attached_obj</code>: The object the gripper is holding (if any).</li> </ul>"},{"location":"tutorials/custom-tasks/#6-add-task-interface-and-config-entries","title":"6. Add Task Interface and Config Entries","text":"<p>Create an environment interface class and task configuration for your new custom task in <code>momagen/env_interfaces/omnigibson.py</code>:</p> <ul> <li>Search for \"Add new class here for new tasks\" to add your env_interface class.</li> <li>Search for \"Add new task configs here\" to add your task_config.</li> </ul>"},{"location":"tutorials/custom-tasks/#7-annotate-subtask-boundaries","title":"7. Annotate Subtask Boundaries","text":"<p>For each subtask, specify the following:</p> <ul> <li><code>MP_end_step</code>: The simulation step where you want the motion planning to end and \"replay\" to begin.</li> <li><code>subtask_term_step</code>: The simulation step where the subtask ends.</li> </ul> <p>The simulation interval between <code>MP_end_step</code> and <code>subtask_term_step</code> is considered the contact-rich segment and will be replayed.</p> <p>To obtain these values, replay the source demonstration using the following command:</p> <pre><code>python momagen/scripts/prepare_src_dataset.py --dataset momagen/datasets/source_og/{hdf5_name} --env_interface {env_interface e.g. MG_R1PickCup} --env_interface_type omnigibson_bimanual --replay_for_annotation \n</code></pre> <p>Note down the simulation step where you would like the contact-rich part to begin (<code>MP_end_step</code>) and end (<code>subtask_term_step</code>). Here is an example of this process:</p>      Your browser does not support the video tag."},{"location":"tutorials/custom-tasks/#8-generate-processed-hdf5-file","title":"8. Generate Processed HDF5 File","text":"<p>Now, generate the HDF5 file containing the <code>datagen_info</code> key used in the MoMaGen data generation pipeline: <pre><code>python momagen/scripts/prepare_src_dataset.py --dataset momagen/datasets/source_og/{hdf5_name} --env_interface {env_interface e.g. MG_R1PickCup} --env_interface_type omnigibson_bimanual --generate_processed_hdf5\n</code></pre></p>"},{"location":"tutorials/custom-tasks/#9-register-base-config-and-task","title":"9. Register Base Config and Task","text":"<p>Add your base config JSON to <code>BASE_CONFIGS</code> and your task name to <code>TASK_NAMES_MOMAGEN_ONLY</code> in <code>momagen/scripts/generate_configs.py</code>. Then run <pre><code>python momagen/scripts/generate_configs.py\n</code></pre></p>"},{"location":"tutorials/custom-tasks/#10-create-task-class","title":"10. Create Task Class","text":"<p>Finally, create a new class for your task in <code>momagen/configs/omnigibson.py</code>.</p>"},{"location":"tutorials/custom-tasks/#step-5-run-data-generation","title":"Step 5: Run Data Generation","text":"<p>Specify the task and parameters, remember to set your own data path:</p> <pre><code>TASK=picking_up_trash # your custom task name\nDR=0 # can be {0, 1, 2}\nNUM_DEMOS=1\nWORKER_ID=0\nFOLDER=/path/to/data # SPECIFY YOUR OWN PATH HERE\n</code></pre> <p>Then run the generation script:</p> <pre><code>python momagen/scripts/generate_dataset.py \\\n    --config momagen/datasets/configs/demo_src_r1_$TASK\\_task_D$DR.json \\\n    --num_demos $NUM_DEMOS \\\n    --bimanual \\\n    --folder $FOLDER/$TASK/r1_$TASK\\_worker_$WORKER_ID \\\n    --seed $WORKER_ID\n</code></pre>"},{"location":"tutorials/generating-data/","title":"Generating Data","text":"<p>This tutorial will walk you through generating demonstration data using MoMaGen.</p>"},{"location":"tutorials/generating-data/#setup","title":"Setup","text":"<p>Before starting, ensure you have completed the Installation guide.</p> <p>Then, verify your installation is working correctly:</p> <pre><code># Activate your environment\nconda activate momagen\n\n# Test imports\npython -c \"import momagen; import omnigibson; import robomimic; print('\u2713 All imports successful')\"\n</code></pre> <p>Next, copy the MoMaGen-specific task instances into the BEHAVIOR-1K dataset:</p> <pre><code># Create directories\nmkdir -p BEHAVIOR-1K/datasets/2025-challenge-task-instances/scenes/Rs_int/json\n\n# Copy scene instances\ncp momagen/scene_instances/Rs_int/* BEHAVIOR-1K/datasets/2025-challenge-task-instances/scenes/Rs_int/json/\ncp momagen/scene_instances/house_single_floor/* BEHAVIOR-1K/datasets/2025-challenge-task-instances/scenes/house_single_floor/json\n</code></pre>"},{"location":"tutorials/generating-data/#generating-demonstrations","title":"Generating Demonstrations","text":"<p>Let's generate a simple <code>pick_cup</code> demonstration:</p>"},{"location":"tutorials/generating-data/#1-generate-configuration","title":"1. Generate Configuration","text":"<pre><code>python momagen/scripts/generate_configs.py\n</code></pre> <p>This creates configuration files for all available tasks in <code>momagen/datasets/configs/</code>.</p>"},{"location":"tutorials/generating-data/#2-generate-a-single-demo","title":"2. Generate a Single Demo","text":"<p>Specify the task and parameters, remember to set your own data path:</p> <pre><code># Set the task name (choose from available tasks above)\nTASK=pick_cup  # Options: pick_cup, tidy_table, dishes_away, clean_pan\nDR=0\nNUM_DEMOS=1\nWORKER_ID=0\nFOLDER=/path/to/data # SPECIFY YOUR OWN PATH HERE\n</code></pre> <p>Then run the generation script:</p> <pre><code>python momagen/scripts/generate_dataset.py \\\n    --config momagen/datasets/configs/demo_src_r1_$TASK\\_task_D$DR.json \\\n    --num_demos $NUM_DEMOS \\\n    --bimanual \\\n    --folder $FOLDER/$TASK/r1_$TASK\\_worker_$WORKER_ID \\\n    --seed $WORKER_ID\n</code></pre> <p>This will:</p> <ul> <li>Load the pick_cup task configuration</li> <li>Initialize the simulation environment</li> <li>Generate ten demonstration trajectory</li> <li>Save the result to the specified folder</li> </ul>"}]}